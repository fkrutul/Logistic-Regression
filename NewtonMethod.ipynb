{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the required libraries that will help us read and preprocess the data, as well as train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import *\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use a built in pandas function to read in our data and add in column identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['id number', 'clump thickness', 'uniformity of cell size','uniformity of cell shape', 'marignal adhesion'\n",
    "         , 'single epithelial cell size', 'bare nuclei', 'bland chromatin', 'normal nucleoli', 'mitoses', 'class']\n",
    "\n",
    "data = pd.read_csv('breast-cancer-wisconsin.data', names = names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets print the first five rows of the table to see that the columns are properly identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id number  clump thickness  uniformity of cell size  \\\n",
      "0    1000025                5                        1   \n",
      "1    1002945                5                        4   \n",
      "2    1015425                3                        1   \n",
      "3    1016277                6                        8   \n",
      "4    1017023                4                        1   \n",
      "\n",
      "   uniformity of cell shape  marignal adhesion  single epithelial cell size  \\\n",
      "0                         1                  1                            2   \n",
      "1                         4                  5                            7   \n",
      "2                         1                  1                            2   \n",
      "3                         8                  1                            3   \n",
      "4                         1                  3                            2   \n",
      "\n",
      "  bare nuclei  bland chromatin  normal nucleoli  mitoses  class  \n",
      "0           1                3                1        1      2  \n",
      "1          10                3                2        1      2  \n",
      "2           2                3                1        1      2  \n",
      "3           4                3                7        1      2  \n",
      "4           1                3                1        1      2  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we must do some preprocessing on the data, which includes adding in a bias column, and splitting the data set into the \"class\" portion, which the predictions will be made on, and the features portion, which includes the rest of the data set minus the id number. Additionally, we fill in all rows that do not contain a numerical value with 1s. I experimented with using the mean and median for this metric as well, but 1s worked the best in terms of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['bias'] = 1 # bias column\n",
    "\n",
    "y = data['class']  \n",
    "y = y.replace(4, True) # Malignant = true\n",
    "y = y.replace(2, False) # Benign = false\n",
    "y = y.values.reshape(len(y), 1) # Data must reshaped into one-dimensional format\n",
    "\n",
    "n_features = 9\n",
    "columns = data.columns.tolist()\n",
    "columns.remove(\"id number\") \n",
    "columns.remove(\"class\")  \n",
    "X = data[columns] \n",
    "X = (X.replace('?', np.NaN)).astype(float) # Fill ? entries with NaN, and cast values to floats \n",
    "X = X.fillna(0).values # Fill all NaN values with 0s and fetch all values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement the sigmoid function from the provided slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(theta, x):\n",
    "    z = np.dot(x, theta)\n",
    "    return (1.0 / (1 + np.exp(-z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarily, we implement the cross entropy loss function, the gradient, and the hessian based on the notation in the provided slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossfunction(y, mu):\n",
    "    return -np.sum(y * np.log(mu) + (1 - y) * np.log(1 - mu))\n",
    "\n",
    "def gradient(x, y, mu):\n",
    "    return np.dot(x.T, mu - y)\n",
    "\n",
    "def hessian(x, mu):\n",
    "    return np.dot((mu * (1 - mu) * x).T, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we implement the Newton's Method for logistic regression, also using the slides as a general guideline. We use a very small value, 0.0000000001, to denote the epsilon that is required to make sure that convergence is properly defined. Also note that my model has a slight variation, from the slides. Instead of calling sigmoid to find the mu in the previous functions, I've done this only once in the newton function and passed in to the rest of the functions as a parameter in order to preserve space. The end result is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(X_train, y_train):\n",
    "    theta = np.array([[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.]]) # zeros array the size of n_features + 1\n",
    "    err = np.Infinity\n",
    "    Δ = np.Infinity\n",
    "    while abs(Δ) > 0.0000000001: # Small value to define when convergence occurs\n",
    "        mu_train = sigmoid(theta, X_train)\n",
    "        g = gradient(X_train, y_train, mu_train)\n",
    "        h = hessian(X_train, mu_train)\n",
    "        theta = theta - np.dot(np.linalg.inv(h), g)\n",
    "        errorupdate = lossfunction(y_train, mu_train)\n",
    "        Δ = err - errorupdate\n",
    "        err = errorupdate      \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we must implement the accuracy measurement function. To do this, I implemented the classification (mu > 0.5) portion from the course slides, and then divided the number of properly classified elements by the total number of elements to obtain the accuracy score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, mu):\n",
    "    mu = mu > 0.5 \n",
    "    x = (y == mu) # Correctly classified\n",
    "    totlength = len(mu) # Total number of classifications\n",
    "    return np.sum(x)/totlength # Num correct divided by num total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split our data into training and testing sets using scikit learn, call the Newton's Method classifier on it, and record the accuracy. Please note that 10 random train-test splits were performed with a 0.8-0.2 train-test ratio, and the accuracy scores were averaged across all 10 runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: \n",
      "0.9586\n"
     ]
    }
   ],
   "source": [
    "accuracies = [] # initialize accuracy array to store the accuracies from 10 splits\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = True) # 10 splits, 0.8 - 0.2 train-test ratio\n",
    "    theta = newton(X_train, y_train) # Run the logistic regression classifier\n",
    "    mu_test = sigmoid(theta, X_test) # Predict\n",
    "    accuracies.append(accuracy(y_test, mu_test)) #store each accuracy in array  \n",
    "avgaccuracy = np.mean(np.array(accuracies)) # average of all accuracies\n",
    "print('Accuracy: ') \n",
    "print(round(avgaccuracy,4)) # accuracy rounded to 4 digits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
